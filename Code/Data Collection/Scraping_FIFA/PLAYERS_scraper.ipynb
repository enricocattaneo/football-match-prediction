{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Scrape Players Data (from fifaindex.com)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBJECTIVE**: Using the BeautifulSoup package, scrape football players' information about fifa attributes from the fifaindex.com website (only five major european leagues). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from random import uniform\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "from random import choice\n",
    "from urllib3.util import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "import urllib3.exceptions\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Building Functions*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is necessary to make calls using different headers respecting also a sleep time between each call (good practice in web scraping applications). The function also handles some of the possible exceptions that might be risen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(pageURL):\n",
    "    \"\"\"Given a URL, makes requests using different headers and with a sleep time (good practice in web scraping applications). The function also handles some of the possible exceptions that might be risen. \n",
    "    Parameters:\n",
    "    - pageURL: web page's URL\"\"\"\n",
    "    global errors\n",
    "    headers_list = [\n",
    "          {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:77.0) Gecko/20100101 Firefox/77.0\",\n",
    "           \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "           \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "           \"Referer\": \"https://www.google.com/\",\n",
    "           \"DNT\": \"1\",\n",
    "           \"Connection\": \"keep-alive\",\n",
    "           \"Upgrade-Insecure-Requests\": \"1\"\n",
    "           },\n",
    "          {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0\",\n",
    "               \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "               \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "               \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "               \"Referer\": \"https://www.google.com/\",\n",
    "               \"DNT\": \"1\",\n",
    "               \"Connection\": \"keep-alive\",\n",
    "               \"Upgrade-Insecure-Requests\": \"1\"\n",
    "           },\n",
    "          {\"Connection\": \"keep-alive\",\n",
    "               \"DNT\": \"1\",\n",
    "               \"Upgrade-Insecure-Requests\": \"1\",\n",
    "               \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\",\n",
    "               \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "               \"Sec-Fetch-Site\": \"none\",\n",
    "               \"Sec-Fetch-Mode\": \"navigate\",\n",
    "               \"Sec-Fetch-Dest\": \"document\",\n",
    "               \"Referer\": \"https://www.google.com/\",\n",
    "               \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "               \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\"\n",
    "           },\n",
    "          {\"Connection\": \"keep-alive\",\n",
    "               \"Upgrade-Insecure-Requests\": \"1\",\n",
    "               \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\",\n",
    "               \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "               \"Sec-Fetch-Site\": \"same-origin\",\n",
    "               \"Sec-Fetch-Mode\": \"navigate\",\n",
    "               \"Sec-Fetch-User\": \"?1\",\n",
    "               \"Sec-Fetch-Dest\": \"document\",\n",
    "               \"Referer\": \"https://www.google.com/\",\n",
    "               \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "               \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "           }]\n",
    "    #time.sleep(uniform(0, 0.5))\n",
    "    req = requests.Session()\n",
    "    headers = choice(headers_list)\n",
    "    retry = Retry(total=8, connect=8, backoff_factor=2)\n",
    "    req.mount('https://', HTTPAdapter(max_retries=retry))\n",
    "     \n",
    "     \n",
    "    try:\n",
    "        html = req.get(pageURL, headers=headers, timeout=12)\n",
    "    except requests.packages.urllib3.exceptions.MaxRetryError:\n",
    "        errors.append('requests.packages.urllib3.exceptions.MaxRetryError')\n",
    "        html = requests.get('http://google.com/f')\n",
    "    except urllib3.exceptions.MaxRetryError:\n",
    "        errors.append('urllib3.exceptions.MaxRetryError')\n",
    "        html = requests.get('http://google.com/f')\n",
    "    except requests.ConnectionError:\n",
    "        errors.append('requests.ConnectionError')\n",
    "        html = requests.get('http://google.com/f')\n",
    "    except urllib3.exceptions.ConnectionError:\n",
    "        errors.append('urllib3.exceptions.ConnectionError')\n",
    "        html = requests.get('http://google.com/f')\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        errors.append('requests.exceptions.ConnectionError')\n",
    "        html = requests.get('http://google.com/f')\n",
    "    except Exception as ex:\n",
    "        errors.append(str(ex))\n",
    "        html = requests.get('http://google.com/f')\n",
    "          \n",
    "    return html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get All Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a list of the seasons and leagues to consider, this function just create the URL of the fifaindex.com specific webpage to scrape from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_allpages(seasons, championship):\n",
    "    \"\"\"Given a list of the seasons and leagues to consider, this function just creates the URL for fifaindex.com specific webpages (linked to the desired season and league).\n",
    "    Parameters:\n",
    "    - seasons: list formatted considering fifaindex.com season type\n",
    "    - championship: list formatted considering fifaindex.com league type\n",
    "    \"\"\"\n",
    "    global all_pages\n",
    "    for j in seasons:\n",
    "        for k in championship:\n",
    "            for i in range(1,25):\n",
    "                if re.match('^fifa(10|11|12|13|14|15).*$', j):\n",
    "                    Newpage_to_add = 'https://www.fifaindex.com/players/' + j + '/?page=' + str(i) + '&' + k + '&order=desc'\n",
    "                else: \n",
    "                    Newpage_to_add = 'https://www.fifaindex.com/players/' + j + '/?page=' + str(i) + '&gender=0&' + k + '&order=desc' \n",
    "                all_pages.add(Newpage_to_add)\n",
    "    return all_pages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Players' Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a webpage's URL, this function returns all players' URLs included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_playerlinks(page_url): \n",
    "    \"\"\"Given a webpage's URL containing players' links, returns all players' URLs included.\n",
    "    Parameters:\n",
    "    - page_url: page URL\n",
    "    \"\"\"   \n",
    "    global player_links\n",
    "    right_ending = re.findall('\\/fifa.*\\/', page_url)[0]\n",
    "    html = make_request(pageURL = page_url)\n",
    "    bs = BeautifulSoup(html.text, 'lxml')\n",
    "    for link in bs.find_all('a', href=re.compile('^(/player/)')): \n",
    "        if 'href' in link.attrs:\n",
    "            if link.attrs['href'] not in player_links: \n",
    "                player_links.add(re.sub('\\/fifa.*\\/$', right_ending, link.attrs['href'])) \n",
    "    return player_links"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scrape Players' Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a specific player webpage, this function returns the desired information using the BeautifulSoup package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_player(website_page):\n",
    "    \"\"\"Given a specific player fifaindex.com profile, returns the desired information using the BeautifulSoup package.\n",
    "    Parameters:\n",
    "    - website_page: player profile on fifaindex.com\n",
    "    \"\"\"   \n",
    "    # Request\n",
    "    player_html = make_request(pageURL = website_page)\n",
    "    soup = BeautifulSoup(player_html.text, 'lxml')\n",
    "    # Attributes\n",
    "    box1 = soup.find_all('p', class_ = '')\n",
    "    ll = []\n",
    "    for val in box1:\n",
    "        ll.append(val.text.strip())\n",
    "    r = re.compile('(.+ \\d{1,2}$|.+ .+ \\d{1,2}$)')\n",
    "    newlist = list(filter(r.match, ll)) \n",
    "    newlist = [x.replace(' ', '') for x in newlist]\n",
    "    yg = dict(re.findall('^\\D+|\\d+$', x) for x in newlist)\n",
    "    # Name\n",
    "    yg['Name'] = re.sub(' FIFA.+', '', soup.h1.text).strip() # improvable\n",
    "    # Observation Date\n",
    "    for date in soup.select('.dropdown:nth-child(3) .dropdown-toggle'):\n",
    "        yg['ObservationDate'] = date.text.strip()\n",
    "    # Teams\n",
    "    for index, team in enumerate(soup.select('.link-team:nth-child(2)')):\n",
    "        if index == 0: yg['Club'] = team.text.strip()\n",
    "        else: yg['NationalTeam'] = team.text.strip()\n",
    "    # Favorite Position\n",
    "    for index, position in enumerate(soup.select('.pt-3 .position')):\n",
    "        if index == 0: yg['FavoritePosition'] = position.text.strip()\n",
    "    # Value and Wage\n",
    "    eur = soup.find_all('p',class_ = 'data-currency-euro')\n",
    "    for e in eur:\n",
    "        yg[e.text.split()[0]] = int(re.sub(r'[.â‚¬]', '', e.text.split()[1]))\n",
    "    # Body Measurements\n",
    "    metrics = soup.find_all('span',class_ = 'data-units-metric')\n",
    "    for index, metr in enumerate(metrics):\n",
    "        if index == 0: yg['Height']= int(re.sub(' cm', '', metr.text))\n",
    "        elif index == 1: yg['Weight']= int(re.sub(' kg', '', metr.text))\n",
    "    # Overall + Potential\n",
    "    for index, val in enumerate(soup.select('.card-header .rating')):\n",
    "        if index == 0: yg['Overall']= int(val.text.strip())\n",
    "        elif index == 1: yg['Potential']= int(val.text.strip())\n",
    "    # Work Rate\n",
    "    for index, workrate in enumerate(soup.select('.col-sm-6 p:nth-child(8) .float-right')):\n",
    "        if index == 0: yg['WorkRate'] = re.sub(' ', '', workrate.text.strip())       \n",
    "    return yg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Final Functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Open_Create_file(path, seas, champ):\n",
    "    \"\"\" Given the season and leagues formatted lists, return an object with all players profile links.\n",
    "    Parameters:\n",
    "    - path: to check if the wanted object already exists (if it does the function just open the file)\n",
    "    - seas: seasons formatted list\n",
    "    - champ: leagues formatted list\n",
    "    \"\"\"   \n",
    "    global all_pages\n",
    "    if os.path.isfile(path):\n",
    "        j_file = open(path)\n",
    "        all_players_pages = json.load(j_file)\n",
    "        j_file.close()\n",
    "    else: \n",
    "        get_allpages(seasons=seas, championship=champ)\n",
    "        for page in tqdm(all_pages, total=len(all_pages)):\n",
    "            all_players_pages = get_playerlinks(page)\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(list(all_players_pages), f, ensure_ascii=False, indent=4)\n",
    "        f.close()\n",
    "    return all_players_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_list(ALL_team_player_pages):\n",
    "    \"\"\"Given all players profile links, returns final object with info on every player.\n",
    "    Parameters:\n",
    "    - ALL_team_player_pages: every player profile link\n",
    "    \"\"\"  \n",
    "    final_list = []\n",
    "    count = 0\n",
    "    for player_page in tqdm(ALL_team_player_pages, total=len(ALL_team_player_pages)): \n",
    "        try:\n",
    "            final_list.append(scrape_player('https://www.fifaindex.com' + player_page))\n",
    "        except ValueError:\n",
    "            count += 1\n",
    "            final_list.append({'ERROR': player_page})  \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_dataframe(path, seas, champ):\n",
    "    \"\"\"Given the season and leagues formatted lists, returns a Dataframe with all players infos.\n",
    "    Parameters (same as Open_Create_file()):\n",
    "    - path: to check if the wanted object already exists (if it does the function just open the file)\n",
    "    - seas: seasons formatted list\n",
    "    - champ: leagues formatted list\"\"\"\n",
    "    all_players_pages = Open_Create_file(path, seas, champ)\n",
    "    final = get_final_list(all_players_pages)\n",
    "    if len(all_players_pages) == len(final):\n",
    "        print('No Error')\n",
    "    else: \n",
    "        print('!!!Error!!!')\n",
    "    df = pd.DataFrame(final)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final Operations (CUSTOMIZABLE!!)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section all the previously presented functions are used to obtain a csv file with all teams attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "all_pages = set()\n",
    "player_links = set()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Defining Seasons and Leagues*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEAGUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major European Leagues \n",
    "leagues = ['league=31', 'league=13','league=16','league=19','league=53'] \n",
    "# Minor Leagues\n",
    "minor_leagues = ['league=14', 'league=17', 'league=20', 'league=32', 'league=54']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEASONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR ANNUAL DATA \n",
    "years = ['fifa10_6', 'fifa11_7','fifa12_8','fifa13_11','fifa14_12','fifa15_16','fifa16_19','fifa17_75','fifa18_175','fifa19_280','fifa20_358','fifa21_421','fifa22_487'] # If possible always 1st observation in September when the market is closed else August\n",
    "\n",
    "minor_weeks = ['fifa16_63','fifa17_96','fifa17_154','fifa17_165','fifa18_198','fifa18_272','fifa18_275','fifa19_300', 'fifa19_349', 'fifa19_351','fifa20_372','fifa20_410','fifa20_415','fifa21_438','fifa21_479','fifa21_483','fifa22_508','fifa22_548','fifa22_554','fifa23_561']\n",
    "\n",
    "# FOR WEEKLY DATA (season format)\n",
    "weeks = {'fifa10_6', 'fifa11_7', 'fifa12_8', 'fifa12_9', 'fifa13_11', 'fifa14_12', 'fifa15_16'}\n",
    "for i in range(19,60): # FIFA16\n",
    "    weeks.add('fifa16_' + str(i))\n",
    "for j in range(74,144): # FIFA17\n",
    "    weeks.add('fifa17_' + str(j))\n",
    "for k in range(174, 246): # FIFA18\n",
    "    weeks.add('fifa18_' + str(k))\n",
    "for a in range(279, 344): # FIFA19\n",
    "    weeks.add('fifa19_' + str(a))\n",
    "for b in range(354,416): # FIFA20\n",
    "    weeks.add('fifa20_' + str(b))\n",
    "for c in range(420, 465): # FIFA21\n",
    "    weeks.add('fifa21_' + str(c))\n",
    "for d in range(487, 532): # FIFA22\n",
    "    weeks.add('fifa22_' + str(d))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Obtain FIFA Players Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_dataframe(path='../../Data/From_Collection/FIFA_scraped/Links JSON/players_links.json', seas=weeks, champ=[leagues + minor_leagues])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../Data/From_Collection/FIFA_scraped/players_weekly_complete.csv\", encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "73df3d2a648ddfe6e132dd0b2981f8c5ee01eb57f65aaa52301d101a94b0ebb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
